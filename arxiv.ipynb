{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Self-supervised self-supervision by combining deep learning and\n",
      "  probabilistic logic\n",
      "Summary:   Labeling training examples at scale is a perennial challenge in machine\n",
      "learning. Self-supervision methods compensate for the lack of direct\n",
      "supervi...\n",
      "Published: 2020-12-23T04:06:41Z\n",
      "Authors: Hunter Lang, Hoifung Poon\n",
      "---\n",
      "Title: Targeted Self Supervision for Classification on a Small COVID-19 CT Scan\n",
      "  Dataset\n",
      "Summary:   Traditionally, convolutional neural networks need large amounts of data\n",
      "labelled by humans to train. Self supervision has been proposed as a method ...\n",
      "Published: 2020-11-20T03:07:17Z\n",
      "Authors: Nicolas Ewen, Naimul Khan\n",
      "---\n",
      "Title: Analyzing the Sample Complexity of Self-Supervised Image Reconstruction\n",
      "  Methods\n",
      "Summary:   Supervised training of deep neural networks on pairs of clean image and noisy\n",
      "measurement achieves state-of-the-art performance for many image recon...\n",
      "Published: 2023-05-30T14:42:04Z\n",
      "Authors: Tobit Klug, Dogukan Atik, Reinhard Heckel\n",
      "---\n",
      "Title: Better Self-training for Image Classification through Self-supervision\n",
      "Summary:   Self-training is a simple semi-supervised learning approach: Unlabelled\n",
      "examples that attract high-confidence predictions are labelled with their\n",
      "pr...\n",
      "Published: 2021-09-02T08:24:41Z\n",
      "Authors: Attaullah Sahito, Eibe Frank, Bernhard Pfahringer\n",
      "---\n",
      "Title: Self-Supervised Speaker Verification with Simple Siamese Network and\n",
      "  Self-Supervised Regularization\n",
      "Summary:   Training speaker-discriminative and robust speaker verification systems\n",
      "without speaker labels is still challenging and worthwhile to explore. In th...\n",
      "Published: 2021-12-08T18:41:19Z\n",
      "Authors: Mufan Sang, Haoqi Li, Fang Liu, Andrew O. Arnold, Li Wan\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def search_arxiv(keyword, max_results=10):\n",
    "    # Construct the query URL\n",
    "    url = 'http://export.arxiv.org/api/query'\n",
    "    params = {\n",
    "        'search_query': f'all:{keyword}',\n",
    "        'start': 0,\n",
    "        'max_results': max_results\n",
    "    }\n",
    "\n",
    "    # Make the HTTP request to the arXiv API\n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Parse the XML response\n",
    "        root = ET.fromstring(response.content)\n",
    "        \n",
    "        # Namespace to access tags correctly; using the Atom namespace\n",
    "        ns = {'': 'http://www.w3.org/2005/Atom'}\n",
    "        \n",
    "        # Loop through the entries and extract information\n",
    "        for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
    "            title = entry.find('{http://www.w3.org/2005/Atom}title').text\n",
    "            summary = entry.find('{http://www.w3.org/2005/Atom}summary').text\n",
    "            published = entry.find('{http://www.w3.org/2005/Atom}published').text\n",
    "            # Extracting authors\n",
    "            authors = [author.find('{http://www.w3.org/2005/Atom}name').text for author in entry.findall('{http://www.w3.org/2005/Atom}author')]\n",
    "            \n",
    "            print('Title:', title)\n",
    "            print('Summary:', summary[:150] + '...')  # Print the first 150 characters of the summary\n",
    "            print('Published:', published)\n",
    "            print('Authors:', ', '.join(authors))\n",
    "            print('---')\n",
    "    else:\n",
    "        print('Failed to fetch data from arXiv.')\n",
    "\n",
    "# Example usage\n",
    "search_arxiv('self supervised', max_results=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded PDF to data\\Self-similar solutions in cylindrical magneto-hydrodynamic blast waves_  with energy injection at the centre.pdf\n",
      "Title: Self-similar solutions in cylindrical magneto-hydrodynamic blast waves\n",
      "  with energy injection at the centre\n",
      "PDF Link: http://arxiv.org/pdf/2403.16675v1\n",
      "---\n",
      "Downloaded PDF to data\\Who is bragging more online_ A large scale analysis of bragging in_  social media.pdf\n",
      "Title: Who is bragging more online? A large scale analysis of bragging in\n",
      "  social media\n",
      "PDF Link: http://arxiv.org/pdf/2403.16668v1\n",
      "---\n",
      "Downloaded PDF to data\\Phase separation dynamics in a symmetric binary mixture of ultrasoft_  particles.pdf\n",
      "Title: Phase separation dynamics in a symmetric binary mixture of ultrasoft\n",
      "  particles\n",
      "PDF Link: http://arxiv.org/pdf/2403.16663v1\n",
      "---\n",
      "Downloaded PDF to data\\Graph Augmentation for Recommendation.pdf\n",
      "Title: Graph Augmentation for Recommendation\n",
      "PDF Link: http://arxiv.org/pdf/2403.16656v1\n",
      "---\n",
      "Downloaded PDF to data\\CLHA_ A Simple yet Effective Contrastive Learning Framework for Human_  Alignment.pdf\n",
      "Title: CLHA: A Simple yet Effective Contrastive Learning Framework for Human\n",
      "  Alignment\n",
      "PDF Link: http://arxiv.org/pdf/2403.16649v1\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import re\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    \"\"\"\n",
    "    Sanitizes filenames to remove characters that are illegal or problematic in file names across various operating systems.\n",
    "    \"\"\"\n",
    "    # Remove illegal characters for filenames\n",
    "    filename = re.sub(r'[<>:\"/\\\\|?*\\n\\r]+', '_', filename)\n",
    "    # Remove leading and trailing whitespaces\n",
    "    filename = filename.strip()\n",
    "    # Shorten the filename to avoid OS limitations\n",
    "    filename = filename[:250]\n",
    "    return filename\n",
    "\n",
    "def download_pdf(url, save_path):\n",
    "    response = requests.get(url)\n",
    "    with open(save_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Downloaded PDF to {save_path}\")\n",
    "\n",
    "def search_arxiv(keyword, max_results=10, sort_by='relevance'):\n",
    "    url = 'http://export.arxiv.org/api/query'\n",
    "    params = {\n",
    "        'search_query': f'all:{keyword}',\n",
    "        'start': 0,\n",
    "        'max_results': max_results,\n",
    "        'sortBy': sort_by,\n",
    "        'sortOrder': 'descending' if sort_by == 'submittedDate' else 'ascending'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        root = ET.fromstring(response.content)\n",
    "        \n",
    "        for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
    "            title = entry.find('{http://www.w3.org/2005/Atom}title').text.strip()\n",
    "            # Extract the link to the PDF\n",
    "            links = entry.findall('{http://www.w3.org/2005/Atom}link')\n",
    "            pdf_link = [link.get('href') for link in links if link.get('type') == 'application/pdf'][0]\n",
    "            \n",
    "            # Sanitize and shorten the filename\n",
    "            pdf_filename = sanitize_filename(title) + '.pdf'\n",
    "            save_path = os.path.join('data', pdf_filename)  # Ensure 'data' directory exists or adjust path as needed\n",
    "            download_pdf(pdf_link, save_path)\n",
    "            \n",
    "            print('Title:', title)\n",
    "            print('PDF Link:', pdf_link)\n",
    "            print('---')\n",
    "    else:\n",
    "        print('Failed to fetch data from arXiv.')\n",
    "\n",
    "# Example usage\n",
    "search_arxiv('self supervised', max_results=5, sort_by='submittedDate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch data from Semantic Scholar.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import re\n",
    "\n",
    "def search_google_scholar(keyword, max_results=10):\n",
    "    \"\"\"\n",
    "    Search Google Scholar for papers related to the keyword and sort by citation count.\n",
    "    This is a hypothetical function and does not actually query Google Scholar due to their restrictions.\n",
    "    \"\"\"\n",
    "    # Hypothetical URL and params for Google Scholar search\n",
    "    url = 'https://api.google.scholar.com/search'\n",
    "    params = {\n",
    "        'q': keyword,\n",
    "        'max_results': max_results,\n",
    "        'sort_by': 'citations',  # Hypothetical parameter for sorting by citations\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        papers = response.json()['papers']  # Hypothetical structure\n",
    "\n",
    "        for paper in papers:\n",
    "            title = paper['title']\n",
    "            pdf_link = paper.get('pdf_link', None)  # Hypothetical direct link to PDF\n",
    "            if pdf_link:\n",
    "                pdf_filename = sanitize_filename(title) + '.pdf'\n",
    "                save_path = os.path.join('data', pdf_filename)\n",
    "                download_pdf(pdf_link, save_path)\n",
    "\n",
    "            print(f\"Title: {title}\")\n",
    "            print(f\"Citations: {paper['citations']}\")\n",
    "            if pdf_link:\n",
    "                print(f\"PDF Link: {pdf_link}\")\n",
    "            print('---')\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to fetch data from Google Scholar: {e}\")\n",
    "\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    filename = re.sub(r'[<>:\"/\\\\|?*\\n\\r]+', '_', filename)\n",
    "    filename = filename.strip()\n",
    "    filename = filename[:250]\n",
    "    return filename\n",
    "\n",
    "def download_pdf(url, save_path):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded PDF to {save_path}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to download {url}: {e}\")\n",
    "\n",
    "def search_semantic_scholar(keyword, max_results=5):\n",
    "    url = 'https://api.semanticscholar.org/graph/v1/paper/search'\n",
    "    params = {\n",
    "        'query': keyword,\n",
    "        'limit': max_results,\n",
    "        'fields': 'title,arxivId,citationCount,url'\n",
    "    }\n",
    "    headers = {\n",
    "        'x-api-key': 'fIfWm6EQgm6SAtyzLec2B2w4L6T4q9iN41raJ9pn'  # Replace with your actual API key\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        for paper in data['data']:\n",
    "            title = paper['title']\n",
    "            citation_count = paper['citationCount']\n",
    "            arxiv_id = paper.get('arxivId', None)\n",
    "            print(f\"Title: {title}\")\n",
    "            print(f\"Citations: {citation_count}\")\n",
    "\n",
    "            if arxiv_id:\n",
    "                pdf_url = f\"http://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
    "                pdf_filename = sanitize_filename(title) + '.pdf'\n",
    "                save_path = os.path.join('data', pdf_filename)\n",
    "                download_pdf(pdf_url, save_path)\n",
    "            else:\n",
    "                print(\"No arXiv link available for this paper.\")\n",
    "            print('---')\n",
    "    else:\n",
    "        print('Failed to fetch data from Semantic Scholar.')\n",
    "\n",
    "# Example usage\n",
    "search_semantic_scholar('quantum computing')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
